version: '3.8'

services:
  # API service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    image: retail-analytics-api
    container_name: retail-analytics-api
    command: api
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - retail-network

  # Dashboard service
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile
    image: retail-analytics-dashboard
    container_name: retail-analytics-dashboard
    command: dashboard
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - api
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - retail-network

  # Vector database for RAG
  vector-db:
    image: milvusdb/milvus:2.3.1
    container_name: retail-analytics-vector-db
    ports:
      - "19530:19530"
      - "19121:19121"
    volumes:
      - ./data/milvus:/var/lib/milvus
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
    networks:
      - retail-network
    depends_on:
      - etcd
      - minio

  # ETCD for Milvus
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    container_name: retail-analytics-etcd
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ./data/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    networks:
      - retail-network

  # MinIO for Milvus
  minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    container_name: retail-analytics-minio
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - ./data/minio:/data
    command: minio server /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - retail-network

  # MLflow tracking server
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.8.0
    container_name: retail-analytics-mlflow
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow:/mlflow
    command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow/mlflow.db --default-artifact-root file:///mlflow/artifacts
    networks:
      - retail-network

  # Prometheus for monitoring
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: retail-analytics-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - retail-network

  # Grafana for visualization
  grafana:
    image: grafana/grafana:10.0.3
    container_name: retail-analytics-grafana
    ports:
      - "3000:3000"
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus
    networks:
      - retail-network

networks:
  retail-network:
    driver: bridge