{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sales Forecasting and Customer Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sales Forecasting and Customer Segmentation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "import os # Added for directory creation\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('ggplot')\n",
        "sns.set_palette(\"Set2\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Define output directories\n",
        "plot_output_dir = 'notebooks/output_plots_forecast_segment'\n",
        "data_output_dir = 'data/processed'\n",
        "os.makedirs(plot_output_dir, exist_ok=True)\n",
        "os.makedirs(data_output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    retail_df = pd.read_csv('data/raw/retail_sales_data.csv')\n",
        "    print(\"Loaded data/raw/retail_sales_data.csv\")\n",
        "    # reviews_df = pd.read_csv('data/raw/product_reviews.csv') # Loaded but not used\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    print(\"Please ensure 'data/raw/retail_sales_data.csv' exists.\")\n",
        "    exit()\n",
        "\n",
        "# Convert date column to datetime\n",
        "retail_df['date'] = pd.to_datetime(retail_df['date'])\n",
        "\n",
        "# Add temporal features\n",
        "retail_df['year'] = retail_df['date'].dt.year\n",
        "retail_df['month'] = retail_df['date'].dt.month\n",
        "retail_df['day'] = retail_df['date'].dt.day\n",
        "retail_df['day_of_week'] = retail_df['date'].dt.dayofweek\n",
        "retail_df['is_weekend'] = retail_df['day_of_week'].isin([5, 6]).astype(int)\n",
        "retail_df['quarter'] = retail_df['date'].dt.quarter\n",
        "\n",
        "# Handle missing values\n",
        "print(\"\\nHandling missing values...\")\n",
        "# Fill missing weather with mode\n",
        "retail_df['weather'] = retail_df['weather'].fillna(retail_df['weather'].mode()[0])\n",
        "# Fill missing promotion with 'None'\n",
        "retail_df['promotion'] = retail_df['promotion'].fillna('None')\n",
        "# Fill missing special_event with False\n",
        "retail_df['special_event'] = retail_df['special_event'].fillna(False).astype(str) # Ensure consistent type for OHE\n",
        "# Fill missing dominant_age_group with mode\n",
        "retail_df['dominant_age_group'] = retail_df['dominant_age_group'].fillna(retail_df['dominant_age_group'].mode()[0])\n",
        "# Fill missing numerical values with median\n",
        "for col in ['num_customers', 'total_sales', 'online_sales', 'in_store_sales', 'avg_transaction', 'return_rate']:\n",
        "    if col in retail_df.columns: # Check if column exists\n",
        "        retail_df[col] = retail_df[col].fillna(retail_df[col].median())\n",
        "\n",
        "print(\"Missing values check after imputation:\")\n",
        "print(retail_df.isnull().sum().loc[lambda x: x > 0]) # Show only columns still having NaNs\n",
        "\n",
        "# Feature Engineering\n",
        "print(\"\\nPerforming feature engineering...\")\n",
        "\n",
        "# Create lag features for time series\n",
        "def create_lag_features(df, group_cols, target_col, lag_periods):\n",
        "    df_copy = df.copy()\n",
        "    for lag in lag_periods:\n",
        "        df_copy[f'{target_col}_lag_{lag}'] = df_copy.groupby(group_cols)[target_col].shift(lag)\n",
        "    return df_copy\n",
        "\n",
        "# Create rolling window features\n",
        "def create_rolling_features(df, group_cols, target_col, windows, functions):\n",
        "    df_copy = df.copy()\n",
        "    for window in windows:\n",
        "        for func in functions:\n",
        "            func_name = func.__name__ if hasattr(func, '__name__') else str(func)\n",
        "            df_copy[f'{target_col}_roll_{window}_{func_name}'] = df_copy.groupby(group_cols)[target_col].transform(\n",
        "                lambda x: x.shift(1).rolling(window=window, min_periods=1).agg(func))\n",
        "    return df_copy\n",
        "\n",
        "# Sort by date for proper time series analysis\n",
        "retail_df = retail_df.sort_values(['store_id', 'category', 'date'])\n",
        "\n",
        "# Create lag features for total_sales (1, 7, and 14 days)\n",
        "retail_df = create_lag_features(retail_df, ['store_id', 'category'], 'total_sales', [1, 7, 14])\n",
        "\n",
        "# Create rolling window features (7, 14, and 30 days with mean and std)\n",
        "retail_df = create_rolling_features(retail_df, ['store_id', 'category'], 'total_sales', [7, 14, 30], [np.mean, np.std])\n",
        "\n",
        "# Create interaction features - handle potential division by zero\n",
        "retail_df['price_per_customer'] = (retail_df['total_sales'] / retail_df['num_customers']).replace([np.inf, -np.inf], 0).fillna(0)\n",
        "retail_df['online_ratio'] = (retail_df['online_sales'] / retail_df['total_sales']).replace([np.inf, -np.inf], 0).fillna(0)\n",
        "retail_df['weekend_promotion'] = retail_df['is_weekend'] * (retail_df['promotion'] != 'None').astype(int)\n",
        "\n",
        "# Drop rows with NaN values created by lag/roll features\n",
        "initial_rows = len(retail_df)\n",
        "retail_df = retail_df.dropna(subset=[col for col in retail_df.columns if 'lag' in col or 'roll' in col])\n",
        "print(f\"Dropped {initial_rows - len(retail_df)} rows due to NaNs from lag/roll features.\")\n",
        "\n",
        "print(\"Feature engineering completed. New shape:\", retail_df.shape)\n",
        "\n",
        "# Part 1: Sales Forecasting\n",
        "print(\"\\n=== Sales Forecasting Model ===\")\n",
        "\n",
        "# Prepare data for forecasting\n",
        "# Select features for forecasting\n",
        "forecast_features = [\n",
        "    'month', 'day', 'day_of_week', 'is_weekend', 'quarter',\n",
        "    'weather', 'promotion', 'special_event', 'dominant_age_group',\n",
        "    'num_customers', 'online_sales', 'in_store_sales', 'avg_transaction',\n",
        "    'total_sales_lag_1', 'total_sales_lag_7', 'total_sales_lag_14',\n",
        "    'total_sales_roll_7_mean', 'total_sales_roll_14_mean', 'total_sales_roll_30_mean',\n",
        "    'total_sales_roll_7_std', 'total_sales_roll_14_std', 'total_sales_roll_30_std',\n",
        "    'price_per_customer', 'online_ratio', 'weekend_promotion'\n",
        "]\n",
        "# Ensure all selected features exist in the dataframe\n",
        "forecast_features = [f for f in forecast_features if f in retail_df.columns]\n",
        "\n",
        "# Target variable\n",
        "target = 'total_sales'\n",
        "\n",
        "# Prepare categorical features\n",
        "cat_features = ['weather', 'promotion', 'special_event', 'dominant_age_group']\n",
        "cat_features = [f for f in cat_features if f in forecast_features] # Ensure they are in the selected features\n",
        "num_features = [f for f in forecast_features if f not in cat_features]\n",
        "\n",
        "# Create a preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features) # Use sparse_output=False for XGBoost feature names\n",
        "    ], remainder='passthrough') # Keep other columns if any\n",
        "\n",
        "# Create a time series split for validation\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Function to evaluate forecasting model\n",
        "def evaluate_forecast_model(X, y, model, cv):\n",
        "    mae_scores, rmse_scores, r2_scores = [], [], []\n",
        "    # Separate preprocessor and regressor from the pipeline\n",
        "    preprocessor = model.named_steps['preprocessor']\n",
        "    regressor = model.named_steps['regressor']\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(cv.split(X)):\n",
        "        print(f\"  Fold {fold+1}/{cv.get_n_splits()}...\")\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        try:\n",
        "            # Fit preprocessor on training data and transform both train and test\n",
        "            X_train_processed = preprocessor.fit_transform(X_train)\n",
        "            X_test_processed = preprocessor.transform(X_test)\n",
        "            # Fit regressor on processed training data\n",
        "            regressor.fit(X_train_processed, y_train)\n",
        "            y_pred = regressor.predict(X_test_processed)\n",
        "            mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
        "            rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "            r2_scores.append(r2_score(y_test, y_pred))\n",
        "        except Exception as e:\n",
        "            print(f\"    Error in fold {fold+1}: {e}\")\n",
        "            mae_scores.append(np.nan)\n",
        "            rmse_scores.append(np.nan)\n",
        "            r2_scores.append(np.nan)\n",
        "    return {'MAE': np.nanmean(mae_scores), 'RMSE': np.nanmean(rmse_scores), 'R2': np.nanmean(r2_scores)}\n",
        "\n",
        "# Select a sample store and category for demonstration\n",
        "store_id = 'store_1'\n",
        "category = 'Electronics'\n",
        "sample_data = retail_df[(retail_df['store_id'] == store_id) & (retail_df['category'] == category)].copy()\n",
        "\n",
        "if sample_data.empty:\n",
        "    print(f\"No data found for {store_id}, {category} after processing. Skipping forecast.\")\n",
        "    forecast_metrics = {'MAE': np.nan, 'RMSE': np.nan, 'R2': np.nan}\n",
        "    importance_df = pd.DataFrame(columns=['Feature', 'Importance']) # Empty df\n",
        "else:\n",
        "    X = sample_data[forecast_features]\n",
        "    y = sample_data[target]\n",
        "\n",
        "    # Create and evaluate XGBoost model\n",
        "    xgb_model = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, objective='reg:squarederror'))\n",
        "    ])\n",
        "\n",
        "    print(f\"\\nTraining forecasting model for {store_id}, {category}...\")\n",
        "    forecast_metrics = evaluate_forecast_model(X, y, xgb_model, tscv)\n",
        "    print(\"\\nForecasting Model Metrics:\")\n",
        "    for metric, value in forecast_metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    try:\n",
        "        # Fit on the entire sample data to get importance\n",
        "        xgb_model.fit(X, y)\n",
        "        feature_importance = xgb_model.named_steps['regressor'].feature_importances_\n",
        "\n",
        "        # Get feature names after one-hot encoding\n",
        "        # Access the fitted OneHotEncoder to get feature names\n",
        "        ohe_feature_names = list(xgb_model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(cat_features))\n",
        "        # Combine numerical and OHE feature names\n",
        "        all_feature_names = num_features + ohe_feature_names\n",
        "\n",
        "        # Create feature importance DataFrame\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': all_feature_names,\n",
        "            'Importance': feature_importance\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        # Plot feature importance\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
        "        plt.title(f'Top 15 Feature Importance for Sales Forecasting ({store_id}, {category})')\n",
        "        plt.tight_layout()\n",
        "        plot_path = os.path.join(plot_output_dir, 'forecast_feature_importance.png')\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "        print(f\"Saved plot: {plot_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating feature importance: {e}\")\n",
        "        importance_df = pd.DataFrame(columns=['Feature', 'Importance']) # Empty df\n",
        "\n",
        "    # Visualize actual vs predicted values (last 30 days of sample)\n",
        "    if len(X) > 30:\n",
        "        split_point = len(X) - 30\n",
        "        X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
        "        y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n",
        "        try:\n",
        "            # Explicitly fit preprocessor and transform\n",
        "            preprocessor_plot = xgb_model.named_steps['preprocessor']\n",
        "            regressor_plot = xgb_model.named_steps['regressor']\n",
        "            X_train_processed_plot = preprocessor_plot.fit_transform(X_train)\n",
        "            X_test_processed_plot = preprocessor_plot.transform(X_test)\n",
        "            regressor_plot.fit(X_train_processed_plot, y_train) # Fit regressor on processed train data\n",
        "            y_pred = regressor_plot.predict(X_test_processed_plot) # Predict on processed test data\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            # Use date from original df for index if possible\n",
        "            test_dates = sample_data['date'].iloc[split_point:]\n",
        "            plt.plot(test_dates, y_test.values, label='Actual', marker='o', linestyle='-')\n",
        "            plt.plot(test_dates, y_pred, label='Predicted', marker='x', linestyle='--')\n",
        "            plt.title(f'Actual vs Predicted Sales ({store_id}, {category}) - Last 30 Days')\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Total Sales')\n",
        "            plt.legend()\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plot_path = os.path.join(plot_output_dir, 'forecast_actual_vs_predicted.png')\n",
        "            plt.savefig(plot_path)\n",
        "            plt.close()\n",
        "            print(f\"Saved plot: {plot_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating actual vs predicted plot: {e}\")\n",
        "    else:\n",
        "        print(\"Not enough data points (<30) to generate actual vs predicted plot.\")\n",
        "\n",
        "\n",
        "# Part 2: Customer Segmentation\n",
        "print(\"\\n=== Customer Segmentation Analysis ===\")\n",
        "\n",
        "# Aggregate data at the store level for segmentation\n",
        "store_agg_funcs = {\n",
        "    'total_sales': 'mean', 'online_sales': 'mean', 'in_store_sales': 'mean',\n",
        "    'num_customers': 'mean', 'avg_transaction': 'mean', 'return_rate': 'mean',\n",
        "    'online_ratio': 'mean', 'price_per_customer': 'mean', 'is_weekend': 'mean'\n",
        "}\n",
        "# Filter functions based on columns existing in retail_df\n",
        "valid_agg_funcs = {k: v for k, v in store_agg_funcs.items() if k in retail_df.columns}\n",
        "\n",
        "if valid_agg_funcs:\n",
        "    store_features = retail_df.groupby('store_id').agg(valid_agg_funcs).reset_index()\n",
        "\n",
        "    # Calculate additional KPIs if possible\n",
        "    if 'online_sales' in store_features.columns and 'in_store_sales' in store_features.columns:\n",
        "        store_features['online_to_instore_ratio'] = (store_features['online_sales'] / store_features['in_store_sales']).replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "    # Prepare data for clustering\n",
        "    cluster_feature_cols = [col for col in store_features.columns if col != 'store_id']\n",
        "    X_cluster = store_features[cluster_feature_cols]\n",
        "\n",
        "    if not X_cluster.empty:\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_cluster)\n",
        "\n",
        "        # Determine optimal number of clusters using silhouette score\n",
        "        silhouette_scores = []\n",
        "        K = range(2, min(6, len(store_features))) # Limit K range\n",
        "        print(f\"\\nCalculating silhouette scores for K={list(K)}...\")\n",
        "        for k in K:\n",
        "            if k < len(X_scaled): # Ensure k is less than n_samples\n",
        "                try:\n",
        "                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Set n_init explicitly\n",
        "                    cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "                    # Need at least 2 unique labels for silhouette score\n",
        "                    if len(np.unique(cluster_labels)) > 1:\n",
        "                         score = silhouette_score(X_scaled, cluster_labels)\n",
        "                         silhouette_scores.append(score)\n",
        "                         print(f\"  K={k}, Silhouette Score: {score:.4f}\")\n",
        "                    else:\n",
        "                         print(f\"  K={k}, Only 1 cluster found, skipping silhouette score.\")\n",
        "                         silhouette_scores.append(-1) # Assign a low score\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error calculating silhouette for K={k}: {e}\")\n",
        "                    silhouette_scores.append(-1) # Assign a low score\n",
        "            else:\n",
        "                print(f\"  Skipping K={k} (>= number of samples)\")\n",
        "                silhouette_scores.append(-1)\n",
        "\n",
        "        # Plot silhouette scores if any were calculated\n",
        "        if any(s > -1 for s in silhouette_scores):\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            valid_K = [k for i, k in enumerate(K) if silhouette_scores[i] > -1]\n",
        "            valid_scores = [s for s in silhouette_scores if s > -1]\n",
        "            if valid_K:\n",
        "                plt.plot(valid_K, valid_scores, 'bo-')\n",
        "                plt.xlabel('Number of clusters (K)')\n",
        "                plt.ylabel('Silhouette Score')\n",
        "                plt.title('Silhouette Score Method For Optimal K')\n",
        "                plt.grid(True)\n",
        "                plot_path = os.path.join(plot_output_dir, 'optimal_clusters.png')\n",
        "                plt.savefig(plot_path)\n",
        "                plt.close()\n",
        "                print(f\"Saved plot: {plot_path}\")\n",
        "\n",
        "                # Choose optimal number of clusters\n",
        "                optimal_k = valid_K[np.argmax(valid_scores)]\n",
        "                print(f\"Optimal number of clusters based on Silhouette Score: {optimal_k}\")\n",
        "            else:\n",
        "                print(\"Could not determine optimal K from silhouette scores.\")\n",
        "                optimal_k = 2 # Default to 2 clusters if calculation failed\n",
        "                print(f\"Defaulting to K={optimal_k}\")\n",
        "\n",
        "        else:\n",
        "            print(\"Could not calculate any valid silhouette scores.\")\n",
        "            optimal_k = 2 # Default to 2 clusters\n",
        "            print(f\"Defaulting to K={optimal_k}\")\n",
        "\n",
        "\n",
        "        # Apply K-means clustering with optimal k\n",
        "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "        store_features['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "        print(f\"\\nApplied K-Means with K={optimal_k}.\")\n",
        "\n",
        "        # Analyze clusters\n",
        "        cluster_analysis = store_features.groupby('cluster')[cluster_feature_cols].mean().reset_index()\n",
        "        print(\"\\nCluster Analysis (Mean Values):\")\n",
        "        print(cluster_analysis)\n",
        "\n",
        "        # Visualize clusters (Example: Online Ratio vs Price Per Customer)\n",
        "        if 'online_ratio' in store_features.columns and 'price_per_customer' in store_features.columns:\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.scatterplot(data=store_features, x='online_ratio', y='price_per_customer', hue='cluster', palette='viridis', s=100, legend='full')\n",
        "            # Add store labels\n",
        "            for i, row in store_features.iterrows():\n",
        "                plt.text(row['online_ratio'] + 0.001, row['price_per_customer'], row['store_id'], fontsize=9)\n",
        "            plt.xlabel('Online Sales Ratio')\n",
        "            plt.ylabel('Average Price Per Customer')\n",
        "            plt.title(f'Store Clusters (K={optimal_k})')\n",
        "            plt.legend(title='Cluster')\n",
        "            plt.grid(True)\n",
        "            plot_path = os.path.join(plot_output_dir, 'store_clusters.png')\n",
        "            plt.savefig(plot_path)\n",
        "            plt.close()\n",
        "            print(f\"Saved plot: {plot_path}\")\n",
        "        else:\n",
        "            print(\"Skipping cluster scatter plot (missing required columns).\")\n",
        "\n",
        "\n",
        "        # Radar chart for cluster profiles\n",
        "        def radar_chart(cluster_data_means, features_for_radar, optimal_k):\n",
        "            # Normalize the mean data for radar chart (0-1 scaling often works well here)\n",
        "            from sklearn.preprocessing import MinMaxScaler\n",
        "            scaler_radar = MinMaxScaler()\n",
        "            radar_data_scaled = pd.DataFrame(scaler_radar.fit_transform(cluster_data_means[features_for_radar]), columns=features_for_radar)\n",
        "\n",
        "            N = len(features_for_radar)\n",
        "            angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "            angles += angles[:1]\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
        "            plt.xticks(angles[:-1], features_for_radar, color='grey', size=10)\n",
        "            ax.set_rlabel_position(0)\n",
        "            plt.yticks(np.linspace(0, 1, 5), [f\"{i:.1f}\" for i in np.linspace(0, 1, 5)], color=\"grey\", size=8)\n",
        "            plt.ylim(0, 1)\n",
        "\n",
        "            # Plot each cluster\n",
        "            for i in range(optimal_k):\n",
        "                values = radar_data_scaled.iloc[i].values.flatten().tolist()\n",
        "                values += values[:1]\n",
        "                ax.plot(angles, values, linewidth=2, linestyle='solid', label=f\"Cluster {i}\")\n",
        "                ax.fill(angles, values, alpha=0.2)\n",
        "\n",
        "            plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "            plt.title('Cluster Profiles (Normalized)', size=15, y=1.1)\n",
        "            return fig\n",
        "\n",
        "        # Select features for radar chart (ensure they exist)\n",
        "        radar_features = [\n",
        "            'total_sales', 'online_ratio', 'num_customers', 'avg_transaction',\n",
        "            'return_rate', 'price_per_customer'\n",
        "        ]\n",
        "        radar_features = [f for f in radar_features if f in cluster_analysis.columns]\n",
        "\n",
        "        if len(radar_features) >= 3: # Need at least 3 features for radar\n",
        "            print(\"\\nGenerating cluster profile radar chart...\")\n",
        "            try:\n",
        "                radar_fig = radar_chart(cluster_analysis, radar_features, optimal_k)\n",
        "                plot_path = os.path.join(plot_output_dir, 'cluster_profiles_radar.png')\n",
        "                radar_fig.savefig(plot_path, bbox_inches='tight') # Use bbox_inches\n",
        "                plt.close(radar_fig)\n",
        "                print(f\"Saved plot: {plot_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating radar chart: {e}\")\n",
        "        else:\n",
        "            print(\"Skipping radar chart (need at least 3 valid features).\")\n",
        "\n",
        "\n",
        "        # Create cluster profile descriptions\n",
        "        cluster_profiles = []\n",
        "        print(\"\\nCluster Profiles:\")\n",
        "        for i in range(optimal_k):\n",
        "            profile_desc = f\"Cluster {i}: \"\n",
        "            cluster_mean_values = cluster_analysis.iloc[i]\n",
        "            # Simple descriptions based on comparison to overall mean\n",
        "            for col in radar_features: # Use features from radar chart for description\n",
        "                 if col in cluster_mean_values and col in store_features.columns:\n",
        "                     overall_mean = store_features[col].mean()\n",
        "                     cluster_mean = cluster_mean_values[col]\n",
        "                     if cluster_mean > overall_mean * 1.1: # Significantly higher\n",
        "                         profile_desc += f\"High {col.replace('_', ' ')}, \"\n",
        "                     elif cluster_mean < overall_mean * 0.9: # Significantly lower\n",
        "                         profile_desc += f\"Low {col.replace('_', ' ')}, \"\n",
        "            profile_desc = profile_desc.strip(', ') + \".\"\n",
        "            stores_in_cluster = store_features[store_features['cluster'] == i]['store_id'].tolist()\n",
        "            stores_str = \", \".join(stores_in_cluster)\n",
        "            print(profile_desc)\n",
        "            print(f\"   Stores: {stores_str}\")\n",
        "            cluster_profiles.append(profile_desc) # Store description for report\n",
        "            cluster_profiles.append(f\"   Stores: {stores_str}\") # Store stores for report\n",
        "\n",
        "\n",
        "        # Save clustered data\n",
        "        cluster_data_path = os.path.join(data_output_dir, 'store_clusters.csv')\n",
        "        store_features.to_csv(cluster_data_path, index=False)\n",
        "        print(f\"\\nSaved clustered store features: {cluster_data_path}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Skipping clustering due to empty feature set.\")\n",
        "        optimal_k = 0 # Indicate clustering was skipped\n",
        "        cluster_profiles = [\"Clustering skipped due to lack of data.\"]\n",
        "\n",
        "else:\n",
        "    print(\"Skipping clustering due to missing aggregated features.\")\n",
        "    optimal_k = 0 # Indicate clustering was skipped\n",
        "    cluster_profiles = [\"Clustering skipped due to missing aggregated features.\"]\n",
        "\n",
        "\n",
        "# Final Summary\n",
        "print(\"\\n--- Script Finished ---\")\n",
        "print(\"Sales Forecasting and Customer Segmentation Analysis Completed\")\n",
        "if not sample_data.empty:\n",
        "    print(f\"Forecasting model for {store_id}, {category} achieved R² of {forecast_metrics.get('R2', np.nan):.4f}\")\n",
        "if optimal_k > 0:\n",
        "    print(f\"Stores were segmented into {optimal_k} distinct clusters\")\n",
        "print(f\"Visualizations saved to: {plot_output_dir}\")\n",
        "print(f\"Clustered store data saved to: {data_output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
